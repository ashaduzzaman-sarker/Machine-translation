{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use KerasNLP layers to build an encoder-decoder Transformer model, and train it on the English-to-Spanish machine translation task"
      ],
      "metadata": {
        "id": "3mKyGLunbn43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "- **KerasNLP Overview**: Provides essential components for NLP, such as model layers, tokenizers, and metrics, simplifying the construction of NLP pipelines.\n",
        "\n",
        "- **Example Focus**: Demonstrates building an encoder-decoder Transformer model for English-to-Spanish translation using KerasNLP.\n",
        "\n",
        "- **Original Example Reference**: Based on a lower-level example by [fchollet](https://keras.io/examples/nlp/neural_machine_translation_with_keras_nlp/), this version leverages KerasNLP for advanced techniques like subword tokenization and translation quality metrics.\n",
        "\n",
        "- **Key Learning Points**:\n",
        "  - Tokenize text using `keras_nlp.tokenizers.WordPieceTokenizer`.\n",
        "  - Implement a sequence-to-sequence Transformer model with KerasNLP layers.\n",
        "  - Train the model on an English-to-Spanish translation task.\n",
        "  - Generate translations using the top-p decoding strategy with `keras_nlp.samplers`."
      ],
      "metadata": {
        "id": "qGskYLN_cmJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "H68Aw1jEdMEO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c12dmkPtbjwG",
        "outputId": "fe0cb6c6-614c-4a40-852f-d37bfd1abae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.0/572.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade rouge-score\n",
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q --upgrade tensorflow\n",
        "!pip install -q --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "from tensorflow_text.tools.wordpiece_vocab import (\n",
        "    bert_vocab_from_dataset as bert_vocab,\n",
        ")"
      ],
      "metadata": {
        "id": "RMtqw4lRdmgP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Hyperparameter"
      ],
      "metadata": {
        "id": "h7JULQjziO02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1   # Epochs should be at least 10 for convergence\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "ENG_VOCAB_SIZE = 15000\n",
        "SPA_VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8"
      ],
      "metadata": {
        "id": "rocW1alPiS9_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the data English-to-Spanish translation dataset"
      ],
      "metadata": {
        "id": "wqRoMunmis1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname='spa-eng.zip',\n",
        "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "text_file = pathlib.Path(text_file).parent / 'spa-eng' / 'spa.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNEjGIioig_m",
        "outputId": "95f39a2a-d647-4ce3-e259-a82191eebca4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "\u001b[1m2638744/2638744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing the data"
      ],
      "metadata": {
        "id": "1zFA75B9jNT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text data to lowercase\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split('\\n')[:-1]\n",
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "    eng, spa = line.split('\\t')\n",
        "    eng = eng.lower()\n",
        "    spa = spa.lower()\n",
        "    text_pairs.append((eng, spa))"
      ],
      "metadata": {
        "id": "al2WeAJ0jXRm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets take a look at our sentence pair\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiqW-9SakA35",
        "outputId": "ffa77af8-02b5-4686-b66f-06a0fd9065ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('who should i inform?', '¿a quién debería informar?')\n",
            "('this steak is too tough.', 'este filete está demasiado duro.')\n",
            "('he bought a dress for her.', 'él le compró un vestido a ella.')\n",
            "('thank you very much for your thoughtful present.', 'muchas gracias por su considerado obsequio.')\n",
            "('i like to fish in the river.', 'me gusta pescar en el río.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Dataset"
      ],
      "metadata": {
        "id": "VemKg1IpkN9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the data into training set, validation set, and test set\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(f'{len(text_pairs)} total pairs')\n",
        "print(f'{len(train_pairs)} training pairs')\n",
        "print(f'{len(val_pairs)} validation pairs')\n",
        "print(f'{len(test_pairs)} test pairs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzKIndHckQ1m",
        "outputId": "739ce799-4d84-483b-e18d-bdd1a8ee45a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the data\n",
        "\n",
        "Two tokenizer use here:\n",
        "\n",
        "1. For Source English language\n",
        "2. FOr Target Spanish language"
      ],
      "metadata": {
        "id": "coODpHLBk4TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the WordPiece tokenizater\n",
        "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
        "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(2),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "LSN6HSG_k7OP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary has a few special, reserved tokens\n",
        "reserved_tokens = ['[PAD]', '[UNK]', '[START]', '[END]']\n",
        "\n",
        "# Create the English tokenizer\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
        "\n",
        "# Create the Spanish tokenizer\n",
        "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)"
      ],
      "metadata": {
        "id": "dCKXdzbxl6--"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see some tokens\n",
        "print(f'English Tokens: {eng_vocab[100:110]}')\n",
        "print(f'Spanish Tokens: {spa_vocab[100:110]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn2z-GAnmhfe",
        "outputId": "87aa9b93-d924-49a7-dd16-1f2c74ce428e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Tokens: ['at', 'know', 'him', 'they', 'there', 'go', 'her', 'has', 'will', 're']\n",
            "Spanish Tokens: ['qué', 'le', 'ella', 'te', 'para', 'mary', 'las', 'más', 'al', 'yo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define the tokenizers\n",
        "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=eng_vocab,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=spa_vocab,\n",
        "    lowercase=False\n",
        ")"
      ],
      "metadata": {
        "id": "r3iBTDkOncxN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try and tokenize a sample from our dataset\n",
        "eng_input_ex = text_pairs[0][0]\n",
        "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
        "print(f'English Sentence: {eng_input_ex}')\n",
        "print(f'Tokens: {eng_tokens_ex}')\n",
        "print('Recovered text after detokenizing:', eng_tokenizer.detokenize(eng_tokens_ex))\n",
        "\n",
        "print()\n",
        "\n",
        "spa_input_ex = text_pairs[0][1]\n",
        "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
        "print(f'Spanish Sentence: {spa_input_ex}')\n",
        "print(f'Tokens: {spa_tokens_ex}')\n",
        "print('Recovered text after detokenizing:', spa_tokenizer.detokenize(spa_tokens_ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DZvliLfnuHn",
        "outputId": "fae79636-a32d-4f7a-8bec-c5f282c189d4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Sentence: she advised him to see the dentist, but he said he didn't have enough time to do so.\n",
            "Tokens: [  83  722  102   67  141   66 2238   10  154   71  181   71  121    8\n",
            "   46   81  303  110   67   77  147   12]\n",
            "Recovered text after detokenizing: tf.Tensor(b\"she advised him to see the dentist , but he said he didn ' t have enough time to do so .\", shape=(), dtype=string)\n",
            "\n",
            "Spanish Sentence: le aconsejó que fuera al dentista, pero él dijo que no tenía tiempo suficiente para hacerlo.\n",
            "Tokens: [ 101  916   80  286  108 2833   13  171   90  164   80   81  213  134\n",
            "  454  104  300   15]\n",
            "Recovered text after detokenizing: tf.Tensor(b'le aconsej\\xc3\\xb3 que fuera al dentista , pero \\xc3\\xa9l dijo que no ten\\xc3\\xada tiempo suficiente para hacerlo .', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Datasets\n",
        "\n",
        "- **Training Dataset Format**:\n",
        "  - **Inputs**:\n",
        "    - A dictionary with two keys:\n",
        "      - `encoder_inputs`: Tokenized source sentence.\n",
        "      - `decoder_inputs`: Target sentence up to word N (used to predict word N+1 and beyond).\n",
        "  - **Target**:\n",
        "    - The target sentence offset by one step, providing the next words for the model to predict.\n",
        "\n",
        "- **Special Tokens**:\n",
        "  - Add `[START]` and `[END]` tokens to the tokenized Spanish input sentence.\n",
        "  - **Padding**: Input is padded to a fixed length using `keras_nlp.layers.StartEndPacker`.\n",
        "\n",
        "- **Objective**: At each training step, predict target words N+1 and beyond using the given inputs and targets."
      ],
      "metadata": {
        "id": "NK-saNlJoqJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(eng, spa):\n",
        "    batch_size = ops.shape(spa)[0]\n",
        "\n",
        "    eng = eng_tokenizer(eng)\n",
        "    spa = spa_tokenizer(spa)\n",
        "\n",
        "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`\n",
        "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        pad_value=eng_tokenizer.token_to_id('[PAD]')\n",
        "    )\n",
        "\n",
        "    eng = eng_start_end_packer(eng)\n",
        "\n",
        "    # Add special tokens ('[START]' and '[END]') to 'spa' and also pad it\n",
        "    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
        "        start_value=spa_tokenizer.token_to_id('[START]'),\n",
        "        end_value=spa_tokenizer.token_to_id('[END]'),\n",
        "        pad_value=spa_tokenizer.token_to_id('[PAD]')\n",
        "    )\n",
        "    spa = spa_start_end_packer(spa)\n",
        "\n",
        "    return(\n",
        "        {\n",
        "            'encoder_inputs': eng,\n",
        "            'decoder_inputs': spa[:, :-1]\n",
        "        },\n",
        "        spa[:, 1:]\n",
        "    )\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "jXDae78wo7GP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at the sequence shapes\n",
        "# (batches of 64 pairs, and all sequences are 40 steps long)\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYG4OY7qqbaO",
        "outputId": "4e8bbbb5-889d-4f5a-a224-8e83bada5a9e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
            "targets.shape: (64, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model\n",
        "\n",
        "- **Model Definition**:\n",
        "  - **Embedding Layer**: Needed to generate a vector for each token in the input sequence. Initialized randomly.\n",
        "  - **Positional Embedding**: Encodes word order in the sequence. Typically, these embeddings are added together.\n",
        "  - **KerasNLP Convenience**: Use `keras_nlp.layers.TokenAndPositionEmbedding` to handle both token and positional embeddings in one step.\n",
        "\n",
        "- **Sequence-to-Sequence Transformer**:\n",
        "  - Composed of `keras_nlp.layers.TransformerEncoder` and `keras_nlp.layers.TransformerDecoder`.\n",
        "  - **Process Flow**:\n",
        "    - **Encoder**: The source sequence is passed through `TransformerEncoder`, which generates a new representation.\n",
        "    - **Decoder**: This representation, along with the target sequence so far (words 0 to N), is fed into `TransformerDecoder` to predict the next words (N+1 and beyond).\n",
        "\n",
        "- **Causal Masking**:\n",
        "  - Necessary to ensure the model only uses past tokens (0 to N) when predicting the next token (N+1).\n",
        "  - **Enabled by Default**: In `keras_nlp.layers.TransformerDecoder`.\n",
        "\n",
        "- **Padding Masking**:\n",
        "  - Padding tokens (\"[PAD]\") must be masked out.\n",
        "  - Set `mask_zero=True` in `keras_nlp.layers.TokenAndPositionEmbedding` to handle this, which will propagate the masking to subsequent layers."
      ],
      "metadata": {
        "id": "2dc14eJQrU_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Block\n",
        "encoder_inputs = keras.Input(shape=(None,), name='encoder_inputs')\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoded_outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM,\n",
        "    num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "\n",
        "encoder = keras.Model(encoder_inputs, encoded_outputs)\n",
        "\n",
        "# Decoder Block\n",
        "decoder_inputs = keras.Input(shape=(None,), name='decoder_inputs')\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name='decoder_state_inputs')\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=SPA_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_nlp.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM,\n",
        "    num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation='softmax')(x)\n",
        "\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs\n",
        "    ],\n",
        "    decoder_outputs\n",
        ")\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoded_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [\n",
        "        encoder_inputs,\n",
        "        decoder_inputs\n",
        "    ],\n",
        "    decoder_outputs,\n",
        "    name='transformer'\n",
        ")"
      ],
      "metadata": {
        "id": "WLBbx_rArPhd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "IOmr--d8xYjE",
        "outputId": "f852ab57-ab94-42aa-d803-b19e7d34f3d2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,850,240\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbeddi…\u001b[0m │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m1,315,072\u001b[0m │ token_and_position_em… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_1 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)    │      \u001b[38;5;34m9,283,992\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                           │                        │                │ transformer_encoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,850,240</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbeddi…</span> │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,072</span> │ token_and_position_em… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,283,992</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                           │                        │                │ transformer_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "fwPbMAI-wlK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    'rmsprop',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7H0E_tzwjpe",
        "outputId": "13327a36-c4f3-422e-d6b1-bb401752aa56"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7962s\u001b[0m 6s/step - accuracy: 0.8211 - loss: 1.4492 - val_accuracy: 0.9820 - val_loss: 0.1529\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bad8e6897e0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding Test Sentences (Qualitative Analysis)"
      ],
      "metadata": {
        "id": "KctMBPcYxEte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequences(input_sentences):\n",
        "    batch_size = 1\n",
        "\n",
        "    # Tokenize the encoder input\n",
        "    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
        "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
        "        pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
        "        encoder_input_tokens = ops.concatenate(\n",
        "            [encoder_input_tokens.to_tensor(), pads], 1\n",
        "        )\n",
        "\n",
        "    # Define a function that outputs the next token's probabilities given the input sequence\n",
        "    def next(prompt, cache, index):\n",
        "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
        "        # Ignore hidden states for now; only needed for contrastive search.\n",
        "        hidden_states = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    # Build a prompt of length 40 with a start token and padding tokens\n",
        "    length = 40\n",
        "    start = ops.full((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n",
        "    pad = ops.full((batch_size, length - 1), spa_tokenizer.token_to_id(\"[PAD]\"))\n",
        "    prompt = ops.concatenate((start, pad), axis=-1)\n",
        "\n",
        "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
        "        next,\n",
        "        prompt,\n",
        "        stop_token_ids=[spa_tokenizer.token_to_id(\"[END]\")],\n",
        "        index=1,  # Start sampling after start token.\n",
        "    )\n",
        "    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n",
        "    return generated_sentences\n",
        "\n",
        "    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n",
        "    return generated_sentences\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in range(2):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences([input_sentence])\n",
        "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
        "    translated = (\n",
        "        translated.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    print(f\"** Example {i} **\")\n",
        "    print(f\"English: {input_sentence}\")\n",
        "    print(f\"Spanish: {translated}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "UoGD6kLExl7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c55172-f91d-4faa-d8bc-55b531d5452a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Example 0 **\n",
            "English: that's my younger sister's photograph.\n",
            "Spanish: ie p que lamento familia piso p estúpido ley\n",
            "\n",
            "** Example 1 **\n",
            "English: my alarm clock didn't go off this morning, so i missed my bus.\n",
            "Spanish: gado somos si izquierda q mary noche de pronto alta he f decidió que aún manzana\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding Test Sentences (Quantitative Analysis)"
      ],
      "metadata": {
        "id": "oGTE1m-y0v-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's compute the ROUGE-1 and ROUGE-2 scores\n",
        "\n",
        "rouge_1 = keras_nlp.metrics.RougeN(order=1)\n",
        "rouge_2 = keras_nlp.metrics.RougeN(order=2)\n",
        "\n",
        "for test_pair in test_pairs[:30]:\n",
        "    input_sentence = test_pair[0]\n",
        "    reference_sentence = test_pair[1]\n",
        "\n",
        "    translated_sentence = decode_sequences([input_sentence])\n",
        "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
        "    translated_sentence = (\n",
        "        translated_sentence.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    rouge_1(reference_sentence, translated_sentence)\n",
        "    rouge_2(reference_sentence, translated_sentence)\n",
        "\n",
        "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
        "print(\"ROUGE-2 Score: \", rouge_2.result())"
      ],
      "metadata": {
        "id": "pVifm5xj0zK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "284a190a-28ed-495b-fbeb-59e19fac73e1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.013737975>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.017252993>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.015082163>}\n",
            "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>}\n"
          ]
        }
      ]
    }
  ]
}